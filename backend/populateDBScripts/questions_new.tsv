UUID	Q#	Trust Index Dimension	Domain applicability	Country applicability	M/O	Group or not group	Question type	Question	Question - Proposed revision	Response indicators	RI - Proposed revision	Score	Points Available	Weighting (low-high)	Reference	Alt Text
QBAEDF	1	N/A	N/A	N/A	M		Tombstone	Title of project		free text		not scored				
QA02D4		N/A	N/A	N/A	M		Tombstone	Project Description		free text		not scored				
Q6C352	2	N/A	N/A	N/A	M		Tombstone	Company or Organization		free text		not scored				
Q0267E	3	N/A	N/A	N/A	M		Tombstone	Industry		CV - Standard Industrial Classification, NAIC, or open source option, include "other"	I realize it might be difficult to implement, but it would be ideal if this had some text search feature. People might know the right words for their industry but that only helps if the first word lines up with the first word in the dropdown.	not scored				
Q381B6	4	N/A	N/A	N/A	M		Tombstone	Country		CV- follow ISO 3166 or open soure option, include "other"		not scored				
Q235F8	5	N/A	N/A	N/A	M		Tombstone	City		free text		not scored				
Q9BAD4 	6	N/A	N/A	N/A	M		Tombstone	Representative	Maybe change this to Point of Contact?	free text		not scored				
Q80D33	7	N/A	N/A	N/A	M		Tombstone	Contact information	Contact information is vague. Would you like an email address? A phone number? I would be more specific about that here.	free text		not scored				
QC2610	8	N/A	N/A	N/A	M		Tombstone	Why are you using AI for this project?	I don't understand the use of Alt Text for this question. It doesn't seem to match the question.	select all that apply:		not scored				Are any features in your model redundant or unnecessary? Use the simplest model that meets your performance goals. - Google Responsible AI Practices
A95B59										Advance operational efficiencies						
AB3515										Improve quality						
A5D57B										Lower transaction costs						
A7CC24										Innovation						
A61317										Augment human processes						
ADD2F9										Improve consistency						
A8117D										other						
QD85E8	9	Bias and Fairness				not group	Risk	Is your user base comprised of individuals or groups from vulnerable populations?					1	high	Reference from AI Principles for Vulnerable Populations in Humanitarian Contexts: There are many recent examples of Artificial Intelligence (AI) systems being used for vulnerable people in humanitarian and disaster response contexts, with serious ethical and security-related implications. In particular, vulnerable populations are put at further risk through biases inherently built into AI systems. There are security concerns regarding their personal information being exposed and even used for persecution purposes. 	Vulnerable Populations: There are several definitions available for the term “vulnerable population”, the words simply imply the disadvantaged sub-segment of the community[1] requiring utmost care, specific ancillary considerations and augmented protections in research. The vulnerable individuals’ freedom and capability to protect one-self from intended or inherent risks is variably abbreviated, from decreased freewill to inability to make informed choices (NCBI)
A00F07										Yes - Most users will be individuals or groups from vulnerable populations		-1				
A96E95										Yes - Some users will be individuals or groups from vulnerable populations		0				
A4952A										No - There are currently no identified vulnerable populations in the user base		1				
Q836F4 	83	Bias and Fairness				not group	Risk	What is the likelihood of an unintended outcome occuring during the operation of your system?					1	medium	Recommended guidance from European Commision Liability for Artificial Intellgence:  Below are listed the most important findings of this report on how liability regimes should be designed – and, where necessary, changed – in order to rise to the challenges emerging digital technologies bring with them.  A person operating a permissible technology that nevertheless carries an increased risk of harm to others, for example AI-driven robots in public spaces, should be subject to strict liability for damage resulting from its operation.  In situations where a service provider ensuring the necessary technical framework has a higher degree of control than the owner or user of an actual product or service equipped with AI, this should be taken into account in determining who primarily operates the technology.  A person using a technology that does not pose an increased risk of harm to others should still be required to abide by duties to properly select, operate, monitor and maintain the technology in use and – failing that – should be liable for breach of such duties if at fault.  A person using a technology which has a certain degree of autonomy should not be less accountable for ensuing harm than if said harm had been caused by a human auxiliary.  Manufacturers of products or digital content incorporating emerging digital technology should be liable for damage caused by defects in their products, even if the defect was caused    Expert Group on Liability for New Technologies by changes made to the product under the producer’s control after it had been placed on the market.  For situations exposing third parties to an increased risk of harm, compulsory liability insurance could give victims better access to compensation and protect potential tortfeasors against the risk of liability.  Where a particular technology increases the difficulties of proving the existence of an element of liability beyond what can be reasonably expected, victims should be entitled to facilitation of proof.  Emerging digital technologies should come with logging features, where appropriate in the circumstances, and failure to log, or to provide reasonable access to logged data, should result in a reversal of the burden of proof in order not be to the detriment of the victim.  The destruction of the victim’s data should be regarded as damage, compensable under specific conditions.  It is not necessary to give devices or autonomous systems a legal personality, as the harm these may cause can and should be attributable to existing persons or bodies.	
A47434										Zero Likelihood - All outcomes of the system are determined in advance, there are no learning components to the system		1				
A81097										Low Likelihood - It is possible that an unexpected event could occur, but the likliehood is low 		0				
A97137										High Likelihood - There are many unknown variables within the system, including how the system learns, as a result, there is a high probability that something unlikely could happen during the operation of the system. 		-1				
QA0F6A		Bias and Fairness				not group	Risk	If an unintended outcome(s) were to occur, who would it impact? 		select all that apply:			1	high	Recommended guidance from European Commision Liability for Artificial Intellgence:  Below are listed the most important findings of this report on how liability regimes should be designed – and, where necessary, changed – in order to rise to the challenges emerging digital technologies bring with them.  A person operating a permissible technology that nevertheless carries an increased risk of harm to others, for example AI-driven robots in public spaces, should be subject to strict liability for damage resulting from its operation.  In situations where a service provider ensuring the necessary technical framework has a higher degree of control than the owner or user of an actual product or service equipped with AI, this should be taken into account in determining who primarily operates the technology.  A person using a technology that does not pose an increased risk of harm to others should still be required to abide by duties to properly select, operate, monitor and maintain the technology in use and – failing that – should be liable for breach of such duties if at fault.  A person using a technology which has a certain degree of autonomy should not be less accountable for ensuing harm than if said harm had been caused by a human auxiliary.  Manufacturers of products or digital content incorporating emerging digital technology should be liable for damage caused by defects in their products, even if the defect was caused  Executive Summary 4 Expert Group on Liability for New Technologies by changes made to the product under the producer’s control after it had been placed on the market.  For situations exposing third parties to an increased risk of harm, compulsory liability insurance could give victims better access to compensation and protect potential tortfeasors against the risk of liability.  Where a particular technology increases the difficulties of proving the existence of an element of liability beyond what can be reasonably expected, victims should be entitled to facilitation of proof.  Emerging digital technologies should come with logging features, where appropriate in the circumstances, and failure to log, or to provide reasonable access to logged data, should result in a reversal of the burden of proof in order not be to the detriment of the victim.  The destruction of the victim’s data should be regarded as damage, compensable under specific conditions.  It is not necessary to give devices or autonomous systems a legal personality, as the harm these may cause can and should be attributable to existing persons or bodies.	
AA35A8										All users equally		-0.5				
AAB8AD										A segment of the population		-1				
A4CF53										Your organization		-0.5				
AD6A9B										A different organization		-0.5				
Q4435F		Bias and Fairness				not group	Risk	What is the potential severity of the effect if one or more of these unintended outcome(s) were to occur?					1	high	Recommended guidance from European Commision Liability for Artificial Intellgence:  Below are listed the most important findings of this report on how liability regimes should be designed – and, where necessary, changed – in order to rise to the challenges emerging digital technologies bring with them.  A person operating a permissible technology that nevertheless carries an increased risk of harm to others, for example AI-driven robots in public spaces, should be subject to strict liability for damage resulting from its operation.  In situations where a service provider ensuring the necessary technical framework has a higher degree of control than the owner or user of an actual product or service equipped with AI, this should be taken into account in determining who primarily operates the technology.  A person using a technology that does not pose an increased risk of harm to others should still be required to abide by duties to properly select, operate, monitor and maintain the technology in use and – failing that – should be liable for breach of such duties if at fault.  A person using a technology which has a certain degree of autonomy should not be less accountable for ensuing harm than if said harm had been caused by a human auxiliary.  Manufacturers of products or digital content incorporating emerging digital technology should be liable for damage caused by defects in their products, even if the defect was caused  Executive Summary 4 Expert Group on Liability for New Technologies by changes made to the product under the producer’s control after it had been placed on the market.  For situations exposing third parties to an increased risk of harm, compulsory liability insurance could give victims better access to compensation and protect potential tortfeasors against the risk of liability.  Where a particular technology increases the difficulties of proving the existence of an element of liability beyond what can be reasonably expected, victims should be entitled to facilitation of proof.  Emerging digital technologies should come with logging features, where appropriate in the circumstances, and failure to log, or to provide reasonable access to logged data, should result in a reversal of the burden of proof in order not be to the detriment of the victim.  The destruction of the victim’s data should be regarded as damage, compensable under specific conditions.  It is not necessary to give devices or autonomous systems a legal personality, as the harm these may cause can and should be attributable to existing persons or bodies.	
A31DFA										High		-1				
A89016										Medium		-0.5				
AED0BB										Low		0				
Q8892C	12	Accountability				not group	Mitigation	Has a risk benefit analysis of all aspects of this system including looking at aspects avoidance, mitigation, transference, and acceptance been completed?	"Has a risk benefit analysis of all aspects of this system been completed including looking at aspects of avoidance, mitigation, transference, and acceptance?"  I see there is a placeholder in the info for definitions of the words above. I was going to suggest definitions be added because this is a pretty heavy question to start out with using terminology that may be unfamiliar to certain groups.   The two "Yes" responses are not mutually exclusive but this is a select-one. I would suggest collapsing the two Yes responses into a single "Yes" response or breaking the two responses out into a series of select-all-that-apply options with the first "Yes" responses options listed out rather than confined to a single answer.				1	medium	Reccommended guidance from Google's Responsible AI Practices:  The use of several metrics rather than a single one will help you to understand tradeoffs between different kinds of errors and experiences.  1) Consider metrics including feedback from user surveys, quantities that track overall system performance and short- and long-term product heath (e.g., click-through rate and customer lifetime value, respectively), and false positive and false negative rates sliced across different subgroups.  2) Ensure that your metrics are appropriate for the context and goals of your system, e.g., a fire alarm system should have high recall, even if that means the occasional false alarm.  There are ISO Standards that can be referenced here, however, we should find best practices that are open.	definitions for  Avoidance:  Mitigation:  Transference:  Acceptance:
A0E748										No		0				
ACE1FC										Yes, we have done analysis including, but not limited to, feedback from user surveys, tracking of system performance, short and long-term product health (eg. click-through rate and customer lifetime values), and false positive and false negative rates sliced across different subgroups.		0.5				
A4DF7E										Yes, we have ensured that the metrics are appropriate for the context (eg. fire alarm systems should have high recall, even if that means the occasional false alarm)		1				
QFB449	11	Accountability				not group	Risk	To what extent is the review of ethics built in to your organization's practice of implementing responsible programs, processes, and technology?					1	low	Capability Maturity Model Intergration: https://cmmiinstitute.com/	
ACFDAF										Initial, there is limited discussion about different trade-offs of the system within our organization.		0				
A99F90										Managed, ethical and responsible processes are planned, documented, performed, monitored, and controlled at the project level. Often reactive.		0.25				
AAEAA9										Defined, processes are well characterized and understood. Processes, standards, procedures, tools, etc are defined at the organizational level. Proactive.		0.5				
A373B0										Quantitatively managed, processes are controlled using statistical and other quantitiative techiques.		0.75				
AEEF81										Optimized, process performance continually improved through incremental and innovative technological improvements.		1				
QBA335	13	Accountability				not group	Mitigation	Is there a level of specialized knowledge required to operate your system? Are users made aware prior to use?					1	medium	Reccommended guidance from IEEE - Ethically Aligned Design:     Competence  1) Creators of A/IS should specify the types and levels of knowledge necessary to understand and operate any given application of A/IS. In specifying the requisite types and levels of expertise, creators should do so for the individual components of A/IS and for the entire systems. 2) Creators of A/IS should integrate safeguards against the incompetent operation of their systems. Safeguards could include issuing 33 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 United States License. notifications/warnings to operators in certain conditions, limiting functionalities for different levels of operators (e.g., novice vs. advanced), system shut-down in potentially risky conditions, etc.  3) Creators of A/IS should provide the parties affected by the output of A/IS with information on the role of the operator, the competencies required, and the implications of operator error. Such documentation should be accessible and understandable to both experts and the general public.  4) Entities that operate A/IS should create documented policies to govern how A/IS should be operated. These policies should include the real-world applications for such A/IS, any preconditions for their effective use, who is qualified to operate them, what training is required for operators, how to measure the performance of the A/IS, and what should be expected from the A/IS. The policies should also include specification of circumstances in which it might be necessary for the operator to override the A/IS.	
AF1882										No, systems is available to all users with all degrees of knowledge		1				
A2C200										Yes, users have been clearly notified in advance that there is a degree of knowledge required to operate the system.		0.5				
A23A35										Yes, any recipient of the outcomes of this system are made aware that a qualified person is operating the system.		0.5			Reccommended guidance from Google's Responsible AI Practices:  Continued monitoring will ensure your model takes real-world performance and user feedback into account. 5) Operators of A/IS should, before operating a system, make sure that they have access to the requisite competencies. The operator need not be an expert in all the pertinent domains but should have access to individuals with the requisite kinds of expertise.  2) Consider both short- and long-term solutions to issues. A simple fix (e.g., blacklisting or whitelisting) may help to solve a problem quickly, but may not be the optimal solution in the long run. Balance short-term simple fixes with longer-term learned solutions.  3) Before updating a deployed model, analyze how the candidate and deployed models differ, and how the update will affect the overall system quality and user experience.	
A92BF7										Yes, users have been notified and the appropriate training has been made available to all users. For internal users, training has been delivered or will be delivered prior to deployment		0.5				
A9C525										Yes, for internal users, training has been delivered or will be delivered prior to deployment		0.5				
AE9C4A										Yes, users won't use the tool if it's too complicated		0				
Q62CB7	15	Accountability				not group	Mitigation	At what stage of development is your organization's risk management process?					1	high	Reccomended guidance from the Montreal Declaration:  2) When the misuse of an AIS endangers public health or safety and has a high probability of occurrence, it is prudent to restrict open access and public dissemination to its algorithm.  5) The errors and ﬂaws discovered in AIS and SAAD should be publicly shared, on a global scale, by public institutions and businesses in sectors that pose a signiﬁcant danger to personal integrity and social organization.  Reccommended guidance from IEEE - Ethically Aligned Design:  Awareness of Misuse 1) Creators should be aware of methods of misuse, and they should design A/IS in ways to minimize the opportunity for these.   2) Raise public awareness around the issues of potential A/IS technology misuse in an informed and measured way by:   - Providing ethics education and security awareness that sensitizes society to the potential risks of misuse of A/IS. For example, provide “data privacy warnings” that some smart devices will collect their users’ personal data.   - Delivering this education in scalable and effective ways, including having experts with the greatest credibility and impact who can minimize unwarranted fear about A/IS.   - Educating government, lawmakers, and enforcement agencies about these issues of A/IS so citizens can work collaboratively with these agencies to understand safe use of A/IS. For example, the same way police officers give public safety lectures in schools, they could provide workshops on safe use and interaction with A/IS.	
A3598E										Does not exist, risk assessment for processes for technology and business decisions do not currently occur.		-1				
AA7EC0										Initial, organization considers risks in an ad hoc manner without following defined processes or polices.		-0.5				
A45E90										Repeatable but intuative, There is an emerging understanding that risks are important and need to be considered. Some approaches to risk assessment exist, but the process is still immature and under development.		0.5				
A7A99E										Defined process, an organization-wide risk management policy defines when and how to conduct risk assessments. Risk assessment follows a defined process that is documented and available to all staff through training.		0.75				
A68762										Managed and measurable, the assessment of risk is a standard procedure and exceptions to following the procedure would be noticed by management. It is likely that risk management is a defined management function with senior level responsiblity. Senior management have determined the levels of risk that the organiation will tolerate and have standards measures for risk/ return ratios.		1				
Q71FC3	16	Bias and Fairness				Group	Mitigation	Does your organization have a review model in place including looking at aspects of diversity and complete representation to ensure alternative perspectives or viewpoints are taken into account in advance of the system operating?		select all that apply:			1	medium	Diversity and inclusion is not only an important aspect of how a business establishes their operations, but how they conduct their busienss, this is a good guide on establishing a Diversity and Inclusion Strategic Plan and Maturity Model within an organization.  Diversity Best Practices Guide: https://www.diversitybestpractices.com/sites/diversitybestpractices.com/files/attachments/2018/01/kaiser_final_part_2_3.pdf.	
A14629										Governance board includes diverse and complete representation including members who represent each area of the organization as well as those with legal and financial responsibilities.		0.2				
AFDC95										Governance board includes a minimum of one individual with reasonable experience and knoweldge in ethics.		0.2				
ADA33F										There is a mechanism and review process for items raised by individuals or groups working on the project to present information such as potential issues, including but not limited to, risks (eg. biases, maturity of process, lack of fairness, etc)		0.2				
A74430										There is a mechanism and review process for credible third parties to review project and comment on any issues, including but not limited, potential risks of the project (eg. biases, maturity of process, lack of fairness, etc)		0.2				
A4C27A										If a third party (eg. government body, civil society organization, etc) or member of the public is aware of the project, there is a mechanism and review process to ensure their question is addressed in a timely manner.		0.2				
AC7524										No review model		0				
Q05AA0	17	Accountability				Group	Risk	What type of technology are you using? 		select all that apply:			1	low	More information coming soon	
A3B229										Image and object recognition: analyzing datasests to automate the recognition, classification, reinforcement, and context associated with an image or object.		1				
A225B1										Text and speech analysis: analyzing datasets to recognize, process, and tag text, speech, voice, and make recommendations based on the tagging.		0.5				
AB729E										Risk assessment: Analyzing datasets to identify patterns and and recommend courses of action and in some cases trigger specific actions.		0.5				
A27287										Content generation: Analyzing dataset to categorize, process, triage, personalize, and serve specific content for specific contexts.		0.5				
AE3D90										Process optimization and workflow automation: Analyzing datasets to identify anomalies, cluster patterns, predict outcomes or ways to optimize and automate specific workflows.		0.5				
A3DC25										Other		0.2				
QE51EB	18	Explainability and Interpretability				Group	Mitigation	Is it possible to discover how your system renders a decision or performs a function?					1	medium	Recommended guidance from IEEE Ethically Aligned Design: Achieving transparency, which may involve a significant portion of the resources required to develop the A/IS, is important to each stakeholder group for the following reasons:  - For users, what the system is doing and why.  - For creators, including those undertaking the validation and certification of A/IS, the systems’ processes and input data.  - For an accident investigator, if accidents occur.  - For those in the legal process, to inform evidence and decision-making.  - For the public, to build confidence in the technology.  Recommended guidance from EU: Explanation (XAI research): A known issue with learning systems based on neural nets is the difficulty to provide clear reasons for the interpretations and decisions of the system. This is due to the fact that the training process has resulted in setting the network parameters to numerical values that are difficult to correlate with the results. In addition, sometimes small changes in some values of the data might result in dramatic changes in the interpretation, leading the system to confuse a school bus with an ostrich for example. This specific issue might be used to deceive the system.   For a system to be trustworthy, it is necessary to be able to understand why it had a given behaviour and why it has provided a given interpretation. It is also necessary to limit these adversarial situations. As of today, this is still an open challenge to AI systems based on neural networks. A whole field of research, Explainable AI (XAI) is trying to address this issue, to better understand the underlying mechanisms and find solutions. The matter is of prime importance not only to explain AI’s behaviour to the developer or the user, but also to simply deploy reliable AI systems.	
A8AB44										System is transparent, it is possible to know for certain how and why the system made a particular decision, or in the case of a robot, acted the way it did.		1				
A58099										System is opaque, it is possible through post-decision or post-action, to analyze through various processes (eg. counterfactual or repeatability testing) to draw an accurate conculsion on how the decision was made or the action was taken.		0.5				
ADD756										System is a black box, but detailed records of the design processes and decision making have been kept throughout the entire process.		0.5				
AABDB4										System is a black box, it is possible to make best guesses on why a decision was rendered or a decision was taken, but it's not certain.		0.5				
QA94E7	19	Robustness				Group	Mitigation	What type(s) of testing have been conducted to ensure quality development of the system?		select all that have been completed:			1	high	Recommended guidance from Google's Responsible AI Practices: Learn from software engineering best test practices and quality engineering to make sure the AI system is working as intended and can be trusted.  1) Conduct rigorous unit tests to test each component of the system in isolation.  2) Conduct integration tests to understand how individual ML components interact with other parts of the overall system.  3) Proactively detect input drift by testing the statistics of the inputs to the AI system to make sure they are not changing in unexpected ways.  4) Use a gold standard dataset to test the system and ensure that it continues to behave as expected. Update this test set regularly in line with changing users and use cases, and to reduce the likelihood of training on the test set.  5) Conduct iterative user testing to incorporate a diverse set of users’ needs in the development cycles.  6) Apply the quality engineering principle of poka-yoke: build quality checks into a system, so that unintended failures either cannot happen or trigger an immediate response (e.g., if an important feature is unexpectedly missing, the AI system won’t output a prediction).	
AD160E										None yet		0				
AEC1A4										Rigorous unit tests to test each component of the system in isolation.		0.125				
A5D66B										Integration tests to understand how individual ML components interact with other parts of the overall system.		0.125				
A4D2FA										Proactive detection to determine drift by testing the statistics of the inputs to the AI system to ensure they are not changing in unexpected ways.		0.125				
A1C8F3										A diverse dataset covering most states of the input variables with many individual examples was used to test the system and ensure that it continues to behave as expected.		0.125				
A6DB38										Edge cases and extreme scenarios have been studied and experimented with to ensure acceptable system performance under all intended use cases of the system		0.125				
AC88AB										Iterative user testing to incorporate a diverse set of user needs in the development cycles.		0.125				
AC5768										Quality engineering principles like poka-yoke have been used to build quality checks into a system so that unintended failures either cannot happen or trigger an immediate response.		0.125				
Q1FB1A	20	Explainability and Interpretability				not group	Mitigation	With which of the following groups are you willing to share information about how your system renders a decision or performs a function in an easy to understand way? Information should include, but not be limited to, data sources, development processess, and consulted stakeholders.					1	low	Recommended guidance from IEEE Ethically Aligned Design:    Develop new standards that describe measurable, testable levels of transparency, so that systems can be objectively assessed and levels of compliance determined. For designers, such standards will provide a guide for self-assessing transparency during development and suggest mechanisms for improving transparency. The mechanisms by which transparency is provided will vary significantly, including but not limited to, the following use cases: - For users of care or domestic robots, a “whydid-you-do-that button” which, when pressed, causes the robot to explain the action it just took. - For validation or certification agencies, the algorithms underlying the A/IS and how they have been verified. - For accident investigators, secure storage of sensor and internal state data comparable to a flight data recorder or black box.	Maps to EU assessment item - "have criteria for deployment for the product been set and made availbale to the user?"
AF650B										The public		1				"The Principle of Explicability: “Operate transparently” Transparency is key to building and maintaining citizen’s trust in the developers of AI systems and AI systems themselves. Both technological and business model transparency matter from an ethical standpoint. Technological transparency implies that AI systems be auditable, comprehensible and intelligible by human beings at varying levels of comprehension and expertise. Business model transparency means that human beings are knowingly informed of the intention of developers and technology implementers of AI systems."
A82BB0										Users who make a special request		0.5				
A32981										Certification bodies for verification of the system		0.25				
A7A3D9										Authorities, if required		0.25				
A48F22										None of the above		0				
Q1C8CF	22	Bias and Fairness				Group	Mitigation	How does the system ensure that rights, values, and principles of the public have been protected through the data collection process?		select all that apply:			1	high	To be referenced somewhere else (w/question about transparency): Recommended guidance from the Montreal Declaration: 6) Access to fundamental resources, knowledge and digital tools must be guaranteed for all.  7) We should support the development of commons algorithms — and of open data needed to train them — and expand their use, as a socially equitable objective.  Recommended guidance from the EU - 1) Respect for (& Enhancement of) Human Autonomy: AI systems should be designed not only to uphold rights, values and principles, but also to protect citizens in all their diversity from governmental and private abuses made possible by AI technology, ensuring a fair distribution of the benefits created by AI technologies, protect and enhance a plurality of human values, and enhance self-determination and autonomy of individual users and communities.   2) Respect for (& Enhancement of) Human Autonomy (Assessment list):   - Is the user informed in case of risks on human mental integrity (nudging) by the product?  - Is useful and necessary information provided to the user of the service/product to enable the latter to take a decision in full self-determination?  - Does the AI system indicate to users that a decision, content, advice, or outcome, is the result of an algorithmic decision of any kind?  - Do users have the facility to interrogate algorithmic decisions in order to fully understand their purpose, provenance, the data relied on, etc.?	
AC637B										The system is equally available to all segments of the potential user base		0.2				
AB26CF										The user is informed on the potential risks on human mental integrity (eg. nudging) when this product is being used.		0.2				
A9E317										Useful information about the design, testing, and development of the system (including, but not limited to, decision criteria, inputs for training the model) are provided in a clear and easy to access manner to the user.		0.2				
A9CF08										Notification is provided in a clear and easy to understand way that there was a machine involved in the creation of a decision, content, advice, outcome, or action.		0.2				
A7DF83										Results of ongoing testing, training, and monitoring of the system are open and available.		0.2				
Q56654	23	Bias and Fairness				not group	Mitigation	Are the objectives of the system clear to the users?					1	low	From EU Transparency Assesment List: "have the usage scenarious for the product been specified and clearly communicated?"	
AC210D										Yes, objectives are clear		0.5				
AEEB13										Yes, clear documentation about these objectives have been provided to the intended user		0.5				
A074BD										No		0				
Q53BC0		Accountability				not group		Are the objectives of the system clear to the operators?					1	medium	We recommend that the system objectives be clearly communicated to the operators of the system 	
ACD980										Yes		1				
A794F6										No		0				
QCDD6C	24	Accountability				not group	Risk	Is there ongoing monitoring of the system to ensure that the system is operating as intended?					1	high	From EU Transparency Assesment List: "what measures are put in place to inform on the products accuracy? On the reasons/ criteria behind outcomes of the product?"	
A7E774										Yes		1				
AB4ED7										No		0				
Q22F17	25	Bias and Fairness				Group	Mitigation	In which of the following areas has the algorithm been designed or trained in a way to avoid the creation, reinforcement, or reproduction of bias:		select all that apply:			1	high	Recommended guidance from the Montreal Declaration:   1) AIS must be designed and trained so as not to create, reinforce, or reproduce discrimination based on — among other things — social, sexual, ethnic, cultural, or religious differences.  2) AIS development must help eliminate relationships of domination between groups and people based on differences of power, wealth, or knowledge.  3) AIS development must produce social and economic beneﬁts for all by reducing social inequalities and vulnerabilities.  Mental Health: Guidance from ALTAI Self-Assessment: Does the AI system risk creating human attachment, stimulating addictive behaviour, or manipulating user behaviour? Depending on which risks are possible or likely, please answer the questions below:     o Did you take measures to deal with possible negative consequences for end-users or subjects in case they develop a disproportionate attachment to the AI System?     o Did you take measures to minimise the risk of addiction?     o Did you take measures to mitigate the risk of manipulation?  	
A89E02										Socio-economic capacity		0.1				
A83E75										Physical attributes		0.1				
AB015E										Level of education		0.1				
A1EC52										Sexual orientation		0.1				
A433A7										Ethnicity		0.1				
A6A457										Cultural association(s)		0.1				
A30DB7										Religious affiliation(s)		0.1				
A1AE82										Mental health		0.1				
A7C3E5										Gender		0.1				
AE6FAD										Age		0.1				
AA93BD										Degree of ability		0.1				
A76B32										other 		0				
QE7CBC	10	Accountability				Group	Risk	What type(s) of unintended outcomes could occur from the use of this system?		select all that apply:			1	medium	Recommended guidance from European Commision Liability for Artificial Intellgence:  Below are listed the most important findings of this report on how liability regimes should be designed – and, where necessary, changed – in order to rise to the challenges emerging digital technologies bring with them.  A person operating a permissible technology that nevertheless carries an increased risk of harm to others, for example AI-driven robots in public spaces, should be subject to strict liability for damage resulting from its operation.  In situations where a service provider ensuring the necessary technical framework has a higher degree of control than the owner or user of an actual product or service equipped with AI, this should be taken into account in determining who primarily operates the technology.  A person using a technology that does not pose an increased risk of harm to others should still be required to abide by duties to properly select, operate, monitor and maintain the technology in use and – failing that – should be liable for breach of such duties if at fault.  A person using a technology which has a certain degree of autonomy should not be less accountable for ensuing harm than if said harm had been caused by a human auxiliary.  Manufacturers of products or digital content incorporating emerging digital technology should be liable for damage caused by defects in their products, even if the defect was caused   4 Expert Group on Liability for New Technologies  For situations exposing third parties to an increased risk of harm, compulsory liability insurance could give victims better access to compensation and protect potential tortfeasors against the risk of liability.  Where a particular technology increases the difficulties of proving the existence of an element of liability beyond what can be reasonably expected, victims should be entitled to facilitation of proof.  Emerging digital technologies should come with logging features, where appropriate in the circumstances, and failure to log, or to provide reasonable access to logged data, should result in a reversal of the burden of proof in order not be to the detriment of the victim.  The destruction of the victim’s data should be regarded as damage, compensable under specific conditions.  It is not necessary to give devices or autonomous systems a legal personality, as the harm these may cause can and should be attributable to existing persons or bodies.	
A5408D										Material		-1				
A7DF45										Moral		-1				
AABD4E										Physical		-1				
A0858D										None		1				
QB09AA	26	Accountability		Will depend on jurisidction -		not group	Mitigation	Were all labour laws or procedures in your jurisdiction followed throughout the development life-cycle of this project?					1	low	Recommended guidance from the Montreal Declaration: 4) Industrial AIS development must be compatible with acceptable working conditions at every step of their life cycle, from natural resources extraction to recycling, and including data processing.  5) The digital activity of users of AIS and digital services should be recognized as labor that contributes to the functioning of algorithms and creates value.	
A14D70										Yes		1				
A8CE69										No		0				
Q7F9D0	27	Data Quality				not group	Risk	How was the data being used for the system collected?					1	medium	Recommended guidance from Google's Responsible AI Practices: 1) Collect and handle data responsibly - Identify whether your ML model can be trained without the use of sensitive data, e.g., by utilizing non-sensitive data collection or an existing public data source. - If it is essential to process sensitive training data, strive to minimize the use of such data. Handle any sensitive data with care: e.g., comply with required laws and standards, provide users with clear notice and give them any necessary controls over data use, follow best practices such as encryption in transit and rest, and adhere to Google privacy principles. - Anonymize and aggregate incoming data using best practice data-scrubbing pipelines: e.g., consider removing personally identifiable information (PII) and outlier or metadata values that might allow de-anonymization (including implicit metadata such as arrival order, removable by random shuffling, as in Prochlo; or the Cloud Data Loss Prevention API to automatically discover and redact sensitive and identifying data).	
AE6FDE										Collected by same organization as development of the system		1				
A77561										Purchased by a dependable third-party		1				
ABBF24										Open dataset with known owner		1				
A8AE9E										Open dataset with unknown origin 		-0.5				
A8F903										Third-party aquisition with an unknown supply chain		-1				
AFF319										Unclear		-1				
Q69E03	28	Data Quality				Group	Risk	Does data used for training or use of the system include personal information?					1	medium	Recommended guidance from the High-Level Expert Group on AI: 1) Respect for Privacy: Privacy and data protection must be guaranteed at all stages of the life cycle of the AI system. This includes all data provided by the user, but also all information generated about the user over the course of his or her interactions with the AI system (e.g. outputs that the AI system generated for specific users, how users responded to particular recommendations, etc.). Digital records of human behaviour can reveal highly sensitive data, not only in terms of preferences, but also regarding sexual orientation, age, gender, religious and political views. The person in control of such information could use this to his/her advantage. Organisations must be mindful of how data is used and might impact users, and ensure full compliance with the GDPR as well as other applicable regulation dealing with privacy and data protection.   2) Respect for Privacy (assessment list):  - If applicable, is the system GDPR compliant?  - Is the personal data information flow in the system under control and compliant with existing privacy protection laws?  - How can users seek information about valid consent and how can such consent be revoked?  - Is it clear, and is it clearly communicated, to whom or to what group issues related to privacy violation can be raised, especially when these are raised by users of, or others affected by, the AI system?	
A106B3										No		1				
A7CC3A										Yes, age		-0.5				
AEBBEB										Yes, name		-0.5				
AFC5B0										Yes, address		-0.5				
A6CC56										Yes, gender		-0.5				
A546F0										Yes, ethnicity		-0.5				
AB47A9										Yes, status		-0.5				
ACEA1C										Yes, degree of accessibility		-0.5				
A7E34E										Yes, level of education		-0.5				
A4D8DE										Yes, income		-0.5				
A3C988										Yes, marital status		-0.5				
A89CAE										Other		-0.5				
Q1577E	29	Bias and Fairness				Group	Mitigation	How is the privacy and intimacy of an individual or group protected in both the development and implementation of your system?		select all that apply:			1	high	Recommended guidance from the Montreal Declaration: 1) Personal spaces in which people are not subjected to surveillance or digital evaluation must be protected from the intrusion of AIS and data acquisition and archiving systems (DAAS).  2) The intimacy of thoughts and emotions must be strictly protected from AIS and DAAS uses capable of causing harm, especially uses that impose moral judgments on people or their lifestyle choices.  3) People must always have the right to digital disconnection in their private lives, and AIS should explicitly offer the option to disconnect at regular intervals, without encouraging people to stay connected.  4) People must have extensive control over information regarding their preferences. AIS must not create individual preference proﬁles to inﬂuence the behavior of the individuals without their free and informed consent.  5) DAAS must guarantee data conﬁdentiality and personal proﬁle anonymity.  6) Every person must be able to exercise extensive control over their personal data, especially when it comes to its collection, use, and dissemination. Access to AIS and digital services by individuals must not be made conditional on their abandoning control or ownership of their personal data.  7) Individuals should be free to donate their personal data to research organizations in order to contribute to the advancement of knowledge.  8) The integrity of one’s personal identity must be guaranteed. AIS must not be used to imitate or alter a person’s appearance, voice, or other individual characteristics in order to damage one’s reputation or manipulate other people.	
AD1BDC										The collection of private information only takes place if a user consents or for authorized surveillance purposes.		0.11				
A90EF1										Data related to personal thoughts and emotions are not used in situations where the system could case harm, especially in circumstances where moral judgements (eg. lifestyle choices) could be made.		0.11				
A7CDEE										Users are able to disconnect or stop sharing information with the system at any point in time. 		0.11				
A44372										Only users have the ability to set profile preferences, changes to the these preferences can easily be done at any time.		0.11				
A01B9E										Access to personal information is limited to only individuals who require it for the direct functioning of the system.		0.11				
A2B090										Individuals have the ability to access their personal data including, but not limited to, the collection, use, and sharing of this data at any time.		0.11				
AACFA1										Individuals have the ability to donate their personal data to research organizations.		0.11				
AA34FF										Data integrity is assured. The system does not use private data to imitate or alter a persons appearance, voice, or other individual characteristics in order to damage one's reputation or manipulate other people.		0.11				
A44302										The system does not curtail people's real or percieved liberty.		0.11				
Q3CFCB	30	Data Quality				not group	Mitigation	Does the system require the use of sensitive data for development or implementation?					1	high	Recommended guidance from Google's Responsible AI Practices: G1) Collect and handle data responsibly - Identify whether your ML model can be trained without the use of sensitive data, e.g., by utilizing non-sensitive data collection or an existing public data source. - If it is essential to process sensitive training data, strive to minimize the use of such data. Handle any sensitive data with care: e.g., comply with required laws and standards, provide users with clear notice and give them any necessary controls over data use, follow best practices such as encryption in transit and rest, and adhere to Google privacy principles. - Anonymize and aggregate incoming data using best practice data-scrubbing pipelines: e.g., consider removing personally identifiable information (PII) and outlier or metadata values that might allow de-anonymization (including implicit metadata such as arrival order, removable by random shuffling, as in Prochlo; or the Cloud Data Loss Prevention API to automatically discover and redact sensitive and identifying data). 	
A7CAC9										Yes		1				
A46211										The use of sensitive data is minimized		0.5				
A87387										No, the use of sensitive data is necessary, however, measures are in place to mitigate the use and sharing of sensitive data 		0				
A30679										No, the use of sensitive data is necessary for the operation of the system 		-1				
QD54A7	31	Data Quality				Group	Mitigation	What steps are followed to ensure data used in training or implementation of this system is handled with care?		select all that apply:			1	high	Recommended guidance from Google's Responsible AI Practices: 1) Collect and handle data responsibly - Identify whether your ML model can be trained without the use of sensitive data, e.g., by utilizing non-sensitive data collection or an existing public data source. - If it is essential to process sensitive training data, strive to minimize the use of such data. Handle any sensitive data with care: e.g., comply with required laws and standards, provide users with clear notice and give them any necessary controls over data use, follow best practices such as encryption in transit and rest, and adhere to Google privacy principles. - Anonymize and aggregate incoming data using best practice data-scrubbing pipelines: e.g., consider removing personally identifiable information (PII) and outlier or metadata values that might allow de-anonymization (including implicit metadata such as arrival order, removable by random shuffling, as in Prochlo; or the Cloud Data Loss Prevention API to automatically discover and redact sensitive and identifying data).  2) Leverage on-device processing where appropriate - If your goal is to learn statistics of individual interactions (e.g., how often certain UI elements are used), consider collecting only statistics that have been computed locally, on-device, rather than raw interaction data, which can include sensitive information. - Consider whether techniques like federated learning, where a fleet of devices coordinates to train a shared global model from locally-stored training data, can improve privacy in your system. - When feasible, apply aggregation, randomization, and scrubbing operations on-device (e.g., Secure aggregation, RAPPOR, and Prochlo's encode step). Note that these operations may only provide pragmatic, best-effort privacy unless the techniques employed are accompanied by proofs.	
A0CF54										Standards for cryptography or security are followed.						
A50AE8										Data is encrypted when in transit or rest.		0.17				
A2AF6F										Anonymize and aggregate incoming data using best-practice data scrubbing pipelines.		0.17				
A0025B										Provides users with clear notification when personal information is being collected, used, or shared.		0.17				
A14E50										When and where possible, system leverages on-device processing.		0.17				
AE3AD4										Where possible, independent data trusts or data collaboratives are used to ensure users privacy.		0.17				
QAF269	32	Bias and Fairness				not group	Risk	If your system have learning capabilities, what protections are in place for safeguarding user privacy?		select all that apply:			1	medium	Recommended guidance from Google's Responsible AI Practices: 3) Appropriately safeguard the privacy of ML models: Because ML models can expose details about their training data via both their internal parameters as well as their externally-visible behavior, it is crucial to consider the privacy impact of how the models were constructed and may be accessed. - Estimate whether your model is unintentionally memorizing or exposing sensitive data using tests based on “exposure” measurements or membership inference assessment. These metrics can additionally be used for regression tests during model maintenance. - Experiment with parameters for data minimization (e.g., aggregation, outlier thresholds, and randomization factors) to understand tradeoffs and identify optimal settings for your model. - Train ML models using techniques that establish mathematical guarantees for privacy. Note that these analytic guarantees are not guarantees about the complete operational system. - Follow best-practice processes established for cryptographic and security-critical software, e.g., the use of principled and provable approaches, peer-reviewed publication of new ideas, open-sourcing of critical software components, and the enlistment of experts for review at all stages of design and development.	
AE5E0E										Not applicable		1				
A30473										Testing has been completed to ensure that the model is not memorizing or exposing sensitive data		0.5				
A4FD04										Use of data is minimized while maintaining		0.5				
Q61060	33	Data Quality				not group	Mitigation	Has training and implementation data been reviewed for quality?		select all that apply:			1	medium	Recommended guidance from the High-Level Group on AI:  1) Data Governance: The quality of the data sets used is paramount for the performance of the trained machine learning solutions. Even if the data is handled in a privacy preserving way, there are requirements that have to be fulfilled in order to have high quality AI. The datasets gathered inevitably contain biases, and one has to be able to prune these away before engaging in training. This may also be done in the training itself by requiring a symmetric behaviour over known issues in the training set. The integrity of the data gathering has to be ensured. Feeding malicious data into the system may change the behaviour of the AI solutions.   2) Data governance (Assessment list):  - Is proper governance of data and process ensured? What process and procedures were followed to ensure proper data governance?  - Is an oversight mechanism put in place? Who is ultimately responsible?  - What data governance regulation and legislation are applicable to the AI system?	
A474D7										No		0				
A12CD2										Yes, data has been reviewed by internal team		0.25				
AF8C49										Yes, data has been reviewed by a qualified third-party		0.75				
A17287										Yes, data is compliant with ISO 8000 or equivalent industry recognized standard		1				
																
Q79336	34	Data Quality				Group	Mitigation	Has your system been trained on data that accurately represents your entire user base?		select all that apply:			1	high	Recommended guidance form Google's Responsible AI Practices: For supervised systems, consider the relationship between the data labels you have, and the items you are trying to predict. If you are using a data label X as a proxy to predict a label Y, in which cases is the gap between X and Y problematic?	
A39A4B										No		0				
A1B141										Yes, sample data used for training and testing accurately reflects user base including, but not limited to, age, location, gender, ethnicity, status, degree of accessibility, level of education, income.		0.2				Alt text: Google Responsible AI Practices - Training-serving skew—the difference between performance during training and performance during serving—is a persistent challenge. During training, try to identify potential skews and work to address them, including by adjusting your training data or objective function. During evaluation, continue to try to get evaluation data that is as representative as possible of the deployed setting.
AD865D										Yes, training data has been ensured that it is fit for purpose.		0.2				
A9C235										Yes, sample data is accurate for current point in time.		0.2				
A8D64C										Yes, sample data has been tested for accuracy.		0.2				
AF3A23										Yes, sample data is complete.		0.2				
Q466C3	35	Data Quality				not group	Mitigation	How is the data being collected, used, and stored being managed?					1	medium	Guidance from IBM's Everday Ethics for AI: 2) Users’ data should be protected from theft, misuse, or data corruption	
AE42C8										Through a third-party data service, terms and conditions are unknown.		0				
A47278										Through a third-party data service, terms and conditions ensure users data should be protected from theft, misuse, or data corruption		1			Guidance from IBM's Everday Ethics for AI: 2) Users’ data should be protected from theft, misuse, or data corruption	
Q214D0	36	Accountability				not group	Mitigation	Is there a robust review of the inputs of your system, including but not limited to the data and algorithms used to train and operate the system?		select all that apply:			1	medium	Recommended guidance from Google's Responsible AI Practices: 1) Plan out your options to pursue interpretability: Pursuing interpretability can happen before, during and after designing and training your model. - What degree of interpretability do you really need? Work closely with relevant domain experts for your model (e.g., healthcare, retail, etc.) to identify what interpretability features are needed, and why. While rare, there are some cases/systems where with sufficient empirical evidence, fine-grain interpretability is not needed. - Can you analyze your training/testing data? For example, if you are working with private data, you may not have access to investigate your input data. - Can you change your training/testing data, for example, gather more training data for certain subsets (e.g., parts/slices of the feature space), or gather test data for categories of interest? - Can you design a new model or are you constrained to an already-trained model? - Are you providing too much transparency, potentially opening up vectors for abuse? - What are your post-train interpretability options? Will you have access to the internals of the model (e.g., black box vs. white box)?  2) Treat interpretability as a core part of the user experience - Iterate with users in the development cycle to test and refine your assumptions about user needs and goals. - Design the UX so that users build useful mental models of the AI system. If not given clear and compelling information, users may make up their own theories about how an AI system works, which can negatively affect how they try to use the system. - Where possible, make it easy for users to do their own sensitivity analysis: empower them to test how different inputs affect the model output. - Additional relevant UX resources: Designing for human needs, user control, teaching an AI, habituation, fairness, representation  3) Design the model to be interpretable - Use the smallest set of inputs necessary for your performance goals to make it clearer what factors are affecting the model. - Use the simplest model that meets your performance goals. - Learn causal relationships not correlations when possible (e.g., use height not age to predict if a kid is safe to ride a roller coaster). - Craft the training objective to match your true goal (e.g., train for the acceptable probability of false alarms, not accuracy). - Constrain your model to produce input-output relationships that reflect domain expert knowledge (e.g., a coffee shop should be more likely to be recommended if it’s closer to the user, if everything else about it is the same).  4) Choose metrics to reflect the end-goal and the end-task - The metrics you consider must address the particular benefits and risks of your specific context. For example, a fire alarm system would need to have high recall, even if that means the occasional false alarm.  5) Understand the trained model: Many techniques are being developed to gain insights into the model (e.g., sensitivity to inputs). - Analyze the model’s sensitivity to different inputs, for different subsets of examples.  6) Communicate explanations to model users - Provide explanations that are understandable and appropriate for the user (e.g., technical details may be appropriate for industry practitioners and academia, while general users may find UI prompts, user-friendly summary descriptions or visualizations more useful). Explanations should be informed by a careful consideration of philosophical, psychological, computer science (including HCI), legal and ethical considerations about what counts as a good explanation in different contexts. - Identify if and where explanations may not be appropriate (e.g., where explanations could result in more confusion for general users, nefarious actors could take advantage of the explanation for system or user abuse, or explanations may reveal proprietary information). - Consider alternatives if explanations are requested by a certain user base but cannot or should not be provided, or if it's not possible to provide a clear, sound explanation. You could instead provide accountability through other mechanisms such as auditing or allow users to contest decisions or to provide feedback to influence future decisions or experiences. - Prioritize explanations that suggest clear actions a user can take to correct inaccurate predictions going forward. - Don’t imply that explanations mean causation unless they do. - Recognize human psychology and limitations (e.g., confirmation bias, cognitive fatigue) - Explanations can come in many forms (e.g., text, graphs, statistics): when using visualization to provide insights, use best practices from HCI and visualization. - Any aggregated summary may lose information and hide details (e.g., partial dependency plots). - The ability to understand the parts of the ML system (especially inputs) and how all the parts work together (“completeness”) helps users to build clearer mental models of the system. These mental models match actual system performance more closely, providing for a more trustworthy experience and more accurate expectations for future learning. - Be mindful of the limitations of your explanations (e.g., local explanations may not generalize broadly, and may provide conflicting explanations of two visually-similar examples). "	
A4B948										Yes, inputs are reviewed internally as part of a general technology review		0.5				
A3DDD3										Yes, inputs are reviewed internally independent of a technology review and include at least one of the following activities: - Analysis of the systems training/testing data - Multiple tests with different training/testing data, for example, gather more training data for certain subsets (e.g., parts/slices of the feature space), or gather test data for categories of interest - Testing of different models  - Testing for right degree of transparency to mitigate any potential abuse - System is built so users can set their own degree of sensitivity - The smallest set of inputs necessary were used to make it clear which factors are affeting the model  - The simplest model to meet the performance goal was used - Model learns on casual relationships not correlations when and where possible  - Model is trained to match true goal  - Model has been constrained to produce input-output relationships that reflect domain expert knowledge - Metrics used address the particular benefits and risks of your specific context  - The model's sensitivity has been trained for different subsets of examples		1				
AD6BCC										Yes, inputs are reviewed by a trusted third-party independent of a technology review		1				
AFE602										No third third-party review is completed		0				
QE7848	37	Accountability		Any country with EU users		not group	Mitigation	Is the personal data information flow in the system compliant with EU GDPR?					1	medium	Simple GDPR checklist: https://gdpr.eu/checklist/	EU GDPR: See https://gdpr.eu/ for GDPR compliance information
A75856										Yes		1				
A52734										No		0				
A79505										Not applicable		1				
QFABC6	38	Accountability		Any system that serves California		not group	Mitigation	Is the personal data information flow in the system compliant with California Privacy Protection?					1	medium	See the California Consumer Privacy Act for more information on regulations: https://oag.ca.gov/privacy/ccpa	California Privacy Protection: See https://oag.ca.gov/privacy/ccpa for CCPA information
AB8401										Yes		1				
A9752E										No		0				
A2CA06										Not applicable		1				
QD3651	39	Bias and Fairness				not group	Mitigation	Was training for diversity and inclusion completed by all individuals working on the design and development of this system?					1	medium	STANDARD: What's acceptable for diversity and inclusion training? At a minimum come up with some criteria.  Guidance from IBM's Everyday Ethics for AI: 1) Make company policies clear and accessible to design and development teams from day one so that no one is confused about issues of responsibility or accountability. As an AI designer or developer, it is your responsibility to know.  AI provides deeper insight into our personal lives when interacting with our sensitive data. As humans are inherently vulnerable to biases, and are responsible for building AI, there are chances for human bias to be embedded in the systems we create. It is the role of a responsible team to minimize algorithmic bias through ongoing research and data collection which is representative of a diverse population.   1) Real-time analysis of AI brings to light both intentional and unintentional biases. When bias in data becomes apparent, the team must investigate and understand where it originated and how it can be mitigated  2) Design and develop without intentional biases and schedule team reviews to avoid unintentional biases. Unintentional biases can include stereotyping, confirmation bias, and sunk cost bias.  3) Instill a feedback mechanism or open dialogue with users to raise awareness of user-identified biases or issues. e.g., Woebot asks “Let me know what you think,” after suggesting a link	
ABBD06										Yes		1				
A39EB2										Yes, only those who worked on the design and input aspects						
A624F7										No		0				
Q4805F	40	Bias and fairness	Targeted selection			not group	Risk	Are psychological, behavioral, geographical or any other societal inferences used for targeting (or other predictions)?		select all that apply:			1		Recommended guidance from Google's Responsible AI Practices: 1) Design your model using concrete goals for fairness and inclusion - Engage with social scientists, humanists, and other relevant experts for your product to understand and account for various perspectives. - Consider how the technology and its development over time will impact different use cases: Whose views are represented? What types of data are represented? What’s being left out What outcomes does this technology enable and how do these compare for different users and communities? What biases, negative experiences, or discriminatory outcomes might occur? - Set goals for your system to work fairly across anticipated use cases: for example, in X different languages, or to Y different age groups. Monitor these goals over time and expand as appropriate. - Design your algorithms and objective function to reflect fairness goals. - Update your training and testing data frequently based on who uses your technology and how they use it.	
A24402										No		1				
A3AB8E										Yes, however we have engaged with social scientists, humanists, and other relevant experts to understand and account for various perspectives, including, but not limited to, how the system will function over time.		0.5				
AF42BB										Yes, however, goals have been set to ensure the system works fairly across anticipated use cases, including across different regions, with different languages, with different segmentations		0.5				
																
QF211C	41	Accountability				not group	Mitigation	Is there a log which determines who has generated and had access to the inputs of the system?		select all that apply:			1	medium	ISO 9001 provides controls for documentation of the system. Helpful checklist to walk you through 9001: https://www.iso-9001-checklist.co.uk/7.5.3-control-of-documented-information-explained.htm	
A202E1										No		0				
A5DE57										Yes, the log identifies the authority or delegated authority responsible		0.5				
A48BA0										Yes, the log records all the recommendations or decisions made by the system and are easy to identify		0.5				
QB6B11	42	Robustness				not group	Mitigation	Is the entire supply chain secure? 					1	high	Recommended guidance from Google's Responsible AI Practices: 3) Keep learning to stay ahead of the curve - Stay up to date on the latest research advances. Research into adversarial machine learning continues to offer improved performance for defenses and some defense techniques are beginning to offer provable guarantees. - Beyond interfering with input, it is possible there may be other vulnerabilities in the ML supply chain. While to our knowledge such an attack has not yet occurred, it is important to consider the possibility and be prepared.	eg. is data or any aspect of the system being stored on the cloud?
A4020B										Yes		1				
AA7ABE										Not sure		0				
A3AA39										No		0				
Q1CE3D		Explainability and Interpretability						Are the terms of reference easy to understand by the intended audience?							We recommend that the terms of reference should be clearly communicated to the intended audience. Purpose, objectives, context, and work plan are established and made clear.  	terms of reference: 
AAA696										Yes						
AE4B46										Not sure						
AF2897 										No						
Q9049E	43	Data Quality				Group	Mitigation	What are the recourse mechanisms for a decision or action that does not meet the objectives of the system?		select all that apply:			1	medium	See response indicators for best practices. We recommend all of these actions be taken. However, we recognise that recourse will be different based on the context, therefore, think about the different users that you have and determine what the best course of action would be to inform them throughout the use of the system when and if there is an issue.	
AB4834										Decision is able to be reversed		0.25				
AD0DD1										Update will be made to the process		0.25				
AD5CFD										If an issue with the system has been identified that could cause harm to an individual or community, it is immediatly taken off-line until it is remediated		0.25				
A23AC6										Users are notified		0.25				
A5F675 										None		0				
QC9FE9	44	Bias and Fairness				not group	Mitigation	Can the deployment of this system restrict an individual from access to a specific business product or service offering?					1	medium	Recommended guidance from the High-Level Expert Group on AI:  1) Design for all: Systems should be designed in a way that allows all citizens to use the products or services, regardless of their age, disability status or social status. It is particularly important to consider accessibility to AI products and services to people with disabilities, which are horizontal category of society, present in all societal groups independent from gender, age or nationality. AI applications should hence not have a one-size-fits-all approach, but be user-centric and consider the whole range of human abilities, skills and requirements. Design for all implies the accessibility and usability of technologies by anyone at any place and at any time, ensuring their inclusion in any living context, thus enabling equitable access and active participation of potentially all people in existing and emerging computer-mediated human activities. This requirement links to the United Nations Convention on the Rights of Persons with Disabilities.   2) Design for all (Assessment list):  - Is the system equitable in use?  - Does the system accommodate a wide range of individual preferences and abilities?  - Is the system usable by those with special needs or disabilities, and how was this designed into the system and how is it verified?  - What definition(s) of fairness is (are) applicable in the context of the system being developed and/or deployed?  - For each measure of fairness applicable, how is it measured and assured?	
A34F49										Yes		0				
AA87C9										No		1				
QC8371	45	Bias and Fairness				not group	Mitigation	Will the user have the ability to challenge decisions/recommendations made by the system?					1	high	We recommend that users are able to question or reverse all automated decisions, espcially in the instance where there is a potential of significant harm.	
AA5148										Yes		1				
A960F9										No		0				
Q41411	46	Bias and Fairness	Mission Statement Characteristics			not group	Mitigation	Does this system impact an end-user or consumer's economic interest, health, access or mobility, licensing and permit issuance, or otherwise impact their lives?					1		Recommended guidance from the Montreal Declaration: 1) AIS must help individuals improve their living conditions, their health, and their working conditions. 5) AIS use should not contribute to increasing stress, anxiety, or a sense of being harassed by one’s digital environment.	
A359B4										Yes		0				
A574CD										No		1				
Q9463F	47	Accountability				not group	Mitigation	Is there a contingency plan in place if the system is not available?					1	high	Recommended guidance from the EU: Good AI governance should include accountability mechanisms, which could be very diverse in choice depending on the goals. Mechanisms can range from monetary compensation (no-fault insurance) to fault finding, to reconciliation without monetary compensations. The choice of accountability mechanisms may also depend on the nature and weight of the activity, as well as the level of autonomy at play. An instance in which a system misreads a medicine claim and wrongly decides not to reimburse may be compensated for with money. In a case of discrimination, however, an explanation and apology might be at least as important.	
A19879										No		0				
A354A1										Yes, there is a contigency plan in place if the system is not available		1				
QFF34A	48	Accountability				not group	Mitigation	Is there a process in place for determining if the automated activity will be flagged for human oversight?		select all that apply:			1	high	Reference guidance from the Montreal declaration: The development and use of AIS must not contribute to lessening the responsibility of human beings when decisions must be made. 1) Only human beings can be held responsible for decisions stemming from recommendations made by AIS, and the actions that proceed therefrom. 2) In all areas where a decision that affects a person’s life, quality of life, or reputation must be made, where time and circumstance permit, the ﬁnal decision must be taken by a human being and that decision should be free and informed 3) The decision to kill must always be made by human beings, and responsibility for this decision must not be transferred to an AIS. 4) People who authorize AIS to commit a crime or an offense, or demonstrate negligence by allowing AIS to commit them, are responsible for this crime or offense. 5) When damage or harm has been inﬂicted by an AIS, and the AIS is proven to be reliable and to have been used as intended, it is not reasonable to place blame on the people involved in its development or use.	
A39F22										System is only used to assist a decision maker		1				
AE62E4										System is replacing a decision that would otherwise be made by a human and no judgement or discretion is required		0.5				
A352B5										System is replacing a decision that would otherwise be made by a human and judgement or discretion is required		0				
Q1B4C3	50	Bias and Fairness				not group	Mitigation	Does training data rely on decisions/outcomes previously made by individuals to influence the outcomes? Was this data reviewed for bias?					1	high	more info coming soon	
A6E936										Yes		1				
A34C7B										No		0				
QA47B7	51	Robustness				not group	Mitigation	Is there tracking of system performance in place, are there established criteria where the system is considered not fit for purpose (eg the usage is outside of what is considered appropriate for the system to provide a decision/ outcome)?					1	low	Recommended guidance from Google's Responsible AI Practices: 2) Machine learning models today are largely a reflection of the patterns of their training data. It is therefore important to communicate the scope and coverage of the training, hence clarifying the capability and limitations of the models. E.g., a shoe detector trained with stock photos can work best with stock photos but has limited capability when tested with user-generated cellphone photos.  3) Communicate limitations to users where possible. For example, an app that uses ML to recognize specific bird species might communicate that the model was trained on a small set of images from a specific region of the world. By better educating the user, you may also improve the feedback provided from users about your feature or application.	
A7084A										Yes		1				
AA137C										No		0				
Q6B15A	52	Bias and Fairness				not group	Risk	The decision or action made by the system has the potential to adversely impact:		select all that apply:			1		Recommended guidance from the Montreal Declaration: 1) AIS must be designed and trained so as not to create, reinforce, or reproduce discrimination based on — among other things — social, sexual, ethnic, cultural, or religious differences.  2) AIS development must help eliminate relationships of domination between groups and people based on differences of power, wealth, or knowledge.  3) AIS development must produce social and economic beneﬁts for all by reducing social inequalities and vulnerabilities.  4) Industrial AIS development must be compatible with acceptable working conditions at every step of their life cycle, from natural resources extraction to recycling, and including data processing.  5) The digital activity of users of AIS and digital services should be recognized as labor that contributes to the functioning of algorithms and creates value.  6) Access to fundamental resources, knowledge and digital tools must be guaranteed for all.  7) We should support the development of commons algorithms — and of open data needed to train them — and expand their use, as a socially equitable objective.	
A22061										Not applicable		1				
A0A5A0										Health and well-being of an individual or group		-1				
AB333A										Economic interests of an individual or group		-1				
A2B512										Ongoing sustainability of an environmental ecosystem		-1			Recommended guidance from the Montreal Declaration: The development and use of AIS must be carried out so as to ensure a strong environmental sustainability of the planet. 1) AIS hardware, its digital infrastructure and the relevant objects on which it relies such as data centers, must aim for the greatest energy efﬁciency and to mitigate greenhouse gas emissions over its entire life cycle.  2) AIS hardware, its digital infrastructure and the relevant objects on which it relies, must aim to generate the least amount of electric and electronic waste and to provide for maintenance, repair, and recycling procedures according to the principles of circular economy.  3) AIS hardware, its digital infrastructure and the relevant objects on which it relies, must minimize our impact on ecosystems and biodiversity at every stage of its life cycle, notably with respect to the extraction of resources and the ultimate disposal of the equipment when it has reached the end of its useful life.  4) Public and private actors must support the environmentally responsible development of AIS in order to combat the waste of natural resources and produced goods, build sustainable supply chains and trade, and reduce global pollution.	
QF1954	54	Robustness				Group	Mitigation	What safeguards have you put in place to ensure your system is robust enough to deal with the edge cases and extreme scenarios (eg. load inputs, adversarial attacks) to adequately mitigate erroneous outcomes to the best extent possible?		select all that apply:			1	high	Recommended guidance from the EU: 2) Robustness (Assessment List): Covered in other questions.  - Resilience to Attack:  # What are the forms of attack to which the AI system is vulnerable? Which of these forms of attack can be mitigated against?  # What systems are in place to ensure data security and integrity? - Reliability & Reproducibility:  # Is a strategy in place to monitor and test that my products or services meet goals, purposes and intended applications?  # Are the used algorithms tested with regards to their reproducibility? Are reproducibility conditions under control? In which specific and sensitive contexts is it necessary to use a different approach?  # For each aspect of reliability and reproducibility that should be considered, how is it measured and assured?  # Are processes for the testing and verification of the reliability of AI systems clearly documented and operationalised to those tasked with developing and testing an AI system?  # What mechanisms can be used to assure users of the reliability of an AI system? - Accuracy through data usage and control:  # What definition(s) of accuracy is (are) applicable in the context of the system being developed and/or deployed?  # For each form of accuracy to be considered how is it measured and assured?  # Is the data comprehensive enough to complete the task in hand? Is the most recent data used (not outdated)?  # What other data sources / models can be added to increase accuracy?  # What other data sources / models can be used to eliminate bias?  # What strategy was put in place to measure inclusiveness of the data? Is the data representative enough of the case to be solved? - Fall-back plan:  # What would be the impact of the AI system failing by: Providing wrong results? Being unavailable?  # Providing societally unacceptable results (e.g. bias)?  # In case of unacceptable impact - Have thresholds and governance for the above scenarios been defined to trigger alternative/fall-back plans?  # Have fall-back plans been defined and tested?	
A79699										None		0				
AF25AF										Have ensured that there is a mitigation plan in place for any individual, group, or organization who has an incentive to make the system misbehave.		0.14				
A8351D										The unintended consequences resulting in a mistake from the system have been assessed and mitigated to the best extent possible.		0.14				
A25314										A rigorous threat model to understand all possible attack vectors has been implemented.		0.14				
AF8ED9										The system has been tested against adversarial attacks.		0.14				
A85213										Third party advesarial testing of the system was completed.		0.14				
AC9302										Ongoing research is being conducted to ensure the latest tools are being applied.		0.14				
AFB89A										Preventative and precautionary measures have been taken.		0.14				
Q4A201	55	Explainability and Interpretability				not group	Mitigation	What information is provided about how the system operates?		select all that apply:			1	low	Recommended guidance from Google's Responsible AI Practices: - Design the UX so that users build useful mental models of the AI system. If not given clear and compelling information, users may make up their own theories about how an AI system works, which can negatively affect how they try to use the system.	
AE6861										Simple counterfactual explanations are provided to users		0.5				
A220E8										Easy to understand visuals		0.5				
AA7355										Access to support team for conversational explanations		0.5				
AF164B										Not available		0				
Q4DF04	56	Explainability and Interpretability				not group	Mitigation	Are the terms and conditions of the system easy to understand?					1	high	We recommend that the terms and conditions of a system be in plain language, digestible, and concise to read to aid with system transparency	
A4DD46										Terms and conditions for the system use plain language and are easy to understand		0.5				
A40789										Terms and conditions take less than thirty minutes to read and understand by target audience		0.5				
AF7383										Terms and conditions are comprehensive and as a result difficult to turn into plain language and take more than thirty minutes to read		0		low		
QC9882	57	Explainability and Interpretability				Group	Risk	To what extent is it possible for the operator of the system to access the training data? 		select all that apply:			1	medium	We recommend that users of the system have full access to the training data in order to help with system explainability. This access should include automatically collected data and key decision parameters of the system. 	
AF6D59										Full access to data, including automated decisioning data, manual changes to data, manual processing of the decision, etc.		0.5				
ADE566										Access to all automatically collected data is available		0.5				
A5D795										Access to all key decision parameters		0.5				
A1E1FE										Some gaps in special cases of the decision (think manual workflows that fall off of the automated workflow where somebody makes a change or makes a one-off decision/edit to the data)		0.25				
A42642										Access to data is limited		0				
A9A98C										No access to data		0				
QFE3D1	58	Explainability and Interpretability				not group	Mitigation	Is it possible to retrain your model and compare results when training data or other factors are changed?					1	medium	We recommend the ability to retrain a model or system through changing various factors such as training data for example. This will allow for a comparison of model performances and keeping an up-to-date model	
A1A89C										Yes		1				
AD9B01										No		0				
QF593F	59	Explainability and Interpretability				not group	Mitigation	Can you gather test data for all categories of interest?					1	low	We recommend having test data available for all categories of interest in order to properly gauge model performance	
A23933										Yes		1				
A7B003										No		0				
QB3F92	60	Explainability and Interpretability				not group	Risk	Can you design a new model or are you constrained to an already trained model?					1	medium	More info coming soon	
A4627D										Yes, a new model can be trained		1				
A45980										No, the existing model must be used, but can be augmented		0.5				
A76900										No, the existing model must be used and can't be augmented		0				
Q9B650	61	Explainability and Interpretability				not group	Risk	Would it be possible for an independent user to change the vectors (parameters) of the model so it is more accurate for their context?					1	medium	More info coming soon	
A5A96B										Yes		1				
AA1E40										No		-1				
A6809E										Not applicable		0				
Q504B1	62	Explainability and Interpretability				Group	Mitigation	To what extent was user testing completed to ensure the goals and objectives of the users are being met? 		select all that apply:			1	high	Recommended guidance from Google's Responsible AI Practices: 3) Model potential adverse feedback early in the design process, followed by specific live testing and iteration for a small fraction of traffic before full deployment. Engage with a diverse set of users and use-case scenarios, and incorporate feedback before and throughout project development. This will build a rich variety of user perspectives into the project and increase the number of people who benefit from the technology.  4) Engage with a diverse set of users and use-case scenarios, and incorporate feedback before and throughout project development. This will build a rich variety of user perspectives into the project and increase the number of people who benefit from the technology.	
A2D8BD										User experience testing has been done since the start of the project		0.2				
AD3D28										User experience testing will be done at every major interval of the project		0.2				
A904D1										User experience testing was done before the launch of the project		0.2				
A10874										Users are able to test how different inputs affect the model output		0.2				
A5335C										Users are able to provide feedback on their representation		0.2				
A61776										No user experience testing was done		0				
Q1F036	63	Data Quality				not group	Mitigation	Do you have clear policies around storing and handling testing data to evaluate model performance such that there is no data leakage to intentionally or unintentionally influence results from model performance and the evaluation of the overall model?					1	low	We recommend that clear policies be established throughout data collection and model creation. Guidance from ODSC - Open Data Science points out that key areas where data leakage can occur are during data collection, data preprocessing and feature engineering, and during data partitioning.	
AE1DC0										Yes		1				
A46205										No		0				
QFE677	64	Data Quality				Group	Mitigation	What regular assessments of the system are done to protect against data drift? 		select all that apply:			1	medium	See response indicators for best practices. We recommend incoporating consistent monitoring of data changes that can cause data drift and take action when they occur. Assessments such as checks to underlying population, analysis of new use cases, and analysis of exceptional cases, can all help protect against data drift.	
A12B80										Checks to the underlying population		0.33				
A290E7										Analysis of new use cases		0.33				
AFE12B										Analysis of exceptional cases		0.33				
AB3ADE										None		0				
ABE768										Other		0.25				
QBA6C6	65	Robustness				Group	Mitigation	What contingencies are incorporated in the model and or system?		select all that apply:			1	medium	See response indicators for best practices. We recommend that these procedures be established and incorporated in the system.	
A00CBB										Established procedures for certain functions are clearly outlined and understood		0.33				
A95B49										There are established procedures for each type of recognized exception (eg denial of service, manual processing, etc.)		0.33				
A5CD30										Regular review of system failures occuring are considered and added to subsequent documentation		0.33				
A58E22										Other		0				
QB71A6	66	Accountability				not group	Mitigation	Does your organization have a framework, standard, or set of controls in place to evaluate responsible AI development and deployment in your organization?					1	high	Recommended guidance from IEEE Ethically Aligned Design: To best respect human rights, society must assure the safety and security of A/IS so that they are designed and operated in a way that benefits humans. Specifically:   1) Governance frameworks, including standards and regulatory bodies, should be established to oversee processes which ensure that the use of A/IS does not infringe upon human rights, freedoms, dignity, and privacy, and which ensure traceability. This will contribute to building public trust in A/IS.   2) A way to translate existing and forthcoming legal obligations into informed policy and technical considerations is needed. Such a method should allow for diverse cultural norms as well as differing legal and regulatory frameworks.  	
AA7C9B										Yes, a framework based on key principles of the organization		0.5				
AE6179										Yes, evaluatable standards or controls		1				
A6F094										No, we leverage a measurable framework from a credible institution		1				
A151C6										No, we haven't developed this yet		0				
Q88620	67	Data Quality				not group	Risk	How was the creation of the dataset funded? 					1	low	Motivation from Datasheets for Datasets: Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. 	
A13747										All data was collected and created by internal researchers		1				
A08F62										It was funded in part or whole by a third party organization including a government or academic institution 		0.5				
A04694										It was funded in part or whole by a third party organization including a research institute, company, or non-governmental organization 		-0.5				
A522FB										Not applicable		1				
QB463D	68	Data Quality				Group	Risk	If data was collected within your organization (or the collection methodology is known), which of the following best practices were followed during data collection? 		select all that apply:			1	low	Motivation from Datasheets for Datasets: How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated?	
AF5BA0										Data was directly observable (eg. raw text, movie ratings)		1				
A6245C										Reported directly by subjects (survey response)		1				
AB9922										Indirectly inferred/ derived from other data (eg. part-of-speech tags, model-based guesses for age or language) 		-0.5				
A8435F										Data that was reported by subjects or indirectly inferred/ derived from other data that was validated and/ or verified. 		-0.5				
AE70B4										If data was automatically collected the mechanisms and procedures were validated 		1				
A226E8										Data collection practices are unknown		-1				
QB683F	69	Data Quality				not group	Mitigation	What is the complexity of the dataset? 					1	low	We recommended achieving a balance bewteen model and dataset complexity (consider the bias-variance tradeoff) to avoid issues such as underfitting and overfitting. Fitting high dimensional data is less computaionally efficient to work with and can be more prone to overfitting.	
AADAD3										The dataset is simple, there are a limited number of instances (eg. documents, photos, people, countries) and types of instances (eg. movies, users, and ratings; people and interactions between them; nodes and edges) that comprise the full dataset. 		1				
A9932B										The dataset is complex, there are a large number of instances (eg. documents, photos, people, countries) and types of instances (eg. movies, users, and ratings; people and interactions between them; nodes and edges) that comprise the full dataset. 		-0.5				
Q8391A	70	Data Quality				not group	Risk	Is your system trained and tested on data that accurately represents the use of the system? That is, is the data fit for purpose?		select all that apply:			1	high	Motivation from Datasheets for Datasets: For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description. Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.	
AFF651										Yes, sample data used for training and testing accurately represents the use of the system		1				
A16D8D										Data has been sampled from a related dataset		0				
ABEB5B										Data being used to train and test the system is aligned, however, some components of the dataset needed to be synthesized to augment for the intended purpose. 		0.5				
A0F048										Not applicable		0				
QF9104	71	Data Quality				not group	Risk 	Is the data being used to train the system "raw" or has it been processed? 					1	low	Recommended guidance from Google's Responsible AI Practices: ML models will reflect the data they are trained on, so analyze your raw data carefully to ensure you understand it. In cases where this is not possible, e.g., with sensitive raw data, understand your input data as much as possible while respecting privacy; for example by computing aggregate, anonymized summaries.  1) Does your data contain any mistakes (e.g., missing values, incorrect labels)?  2) Is your data sampled in a way that represents your users (e.g., will be used for all ages, but you only have training data from senior citizens) and the real-world setting (e.g., will be used year-round, but you only have training data from the summer)? Is the data accurate?  3) Training-serving skew—the difference between performance during training and performance during serving—is a persistent challenge. During training, try to identify potential skews and work to address them, including by adjusting your training data or objective function. During evaluation, continue to try to get evaluation data that is as representative as possible of the deployed setting.  4) Are any features in your model redundant or unnecessary? Use the simplest model that meets your performance goals.  5) For supervised systems, consider the relationship between the data labels you have, and the items you are trying to predict. If you are using a data label X as a proxy to predict a label Y, in which cases is the gap between X and Y problematic?  6) Data bias is another important consideration; learn more in practices on AI and fairness.	
AD6779										Data is "raw" no further processing has been done from direct collection.		1				
AA247A										Data has been processed from it's "raw" state to be interpreted for human consumption, data is representative of "raw" state. 		0.5				
A437D4										Data has been processed from it's "raw" state for other purposes.		0.5				
Q34ACA	72	Data Quality				not group	Risk 	How are data being produced by your system or model being re-used or re-interpreted? 					1	low	more info coming soon	
A070B5										The data is used to recalibrate and retrain the same system or model 		1				
A52CB6 										Data are not re-used or re-interpreted		1				
A811E4										The data from this model or system is collected and made available for reuse within the same organization		0.5				
AEC70C										The data from this model or system is collected and made available for reuse outside of the organization 		0				
Q2BDD8	73	Data Quality				not group	Mitigation	What is the lifecycle of the dataset being used for this system or model?  					1	low	more info coming soon	
A62CF2										Data being used for training, operating, and maintaining the system or model is disposed of after it is no longer needed for operations 		1				
A17C08										Data being used for training, operating, and maintaining the system or model is retained after operations as it may be used for different models or systems		0				
QBEA4F	74	Data Quality				Group	Risk	All datasets being used for this system or model throughout each phase of the lifecycle have been reviewed for completeness following industry best practices (equivalent to ISO 8000 Part 140 and ISO 25012)? 		select all that apply:			1	medium	Dataset Quality Guidelines : AI Global           Completeness: The degree to which subject data associated with an entity has values for all expected attributes and related entity instances in a specific context of use.   The following are within the scope of this part of ISO 8000:    - requirements for capture and exchange of data completeness information in the form of statements and assertions of data completeness;    - conceptual data model for data completeness information in the form of statements and assertions of data completeness.	
AB4095										Data was not reviewed for completeness		-1				
A1BBB7										All values in the dataset have a specific context or use		0.2				
A3C935										There are no missing values that are pertinent for the intended use of the dataset 		0.2				
A7B9D6										Comprehensive metadata including pertinent collection information including who collected the data, ownership, the description of the dataset, the provenance, etc.  		0.2				
A05E0B										There is a data dictionary to support and help interpret the dataset		0.2				
A876E4										Data attributes within dataset conform to a data specification 		0.2				
Q70823	75	Data Quality				Group	Risk	All datasets being used for this system or model throughout each phase of the lifecycle have been reviewed for accuracy following industry best practices (equivalent to ISO 8000 Part 130 and ISO 25012)? 		select all that apply:			1	medium	Dataset Quality Guidelines: AI Global  Accuracy: The degree to which data has attributes that correctly represent the true value of the intended attribute of a concept or event in a specific context of use.    Syntactic Accuracy: Syntactic accuracy is defined as the closeness of the data values to a set of values defined in a domain considered syntactically correct.    Semantic Accuracy: Semantic accuracy is defined as the closeness of the data values to a set of values defined in a domain considered semantically correct.     A common definition of “accuracy” is the degree to which something reflects the real world. The problem with this definition is that the comparison with the real world is rarely absolute. Who makes the claim to accuracy (business identifier) . If the accuracy is covered by a warrantee: The Universal Resource Identifier (URI) where the warrantee is located. If the accuracy is asserted what is the Universal Resource Identifier (URI) where the assertion is located.	
A2FA19										Data was not reviewed for accuracy		-1				
A3F9D2										All values within the dataset represent the true intent of a sepcific event, context, or use		0.25				
A0C4D4										Data has been validated against applicable reference data where possible 		0.25				
AE9879										Dataset contains applicable labels that are representative of the data attributes contained in the dataset 		0.25				
A760AE										Data was collected in a way that accurately represents the intended purpose, population, event, etc		0.25				
Q1DAE3	76	Data Quality				Group	Risk	All datasets being used for this system or model throughout each phase of the lifecycle have been reviewed for credibility/ provenance following industry best practices (equivalent to ISO 8000 Part 130 and ISO 25012)? 		select all that apply:			1	medium	Dataset Quality Guidelines: AI Global     Credibility: The degree to which data has attributes that are regarded as true and believable by users in a specific context of use. Credibility includes the concept of authenticity (the truthfulness of origins, attributions, commitments).    Provenance: “the history of the data”	
ADAE6F										Data was not reviewed for credibility/ provenance		-1				
AA6296										All values included in the dataset are true, accurate, and or authentic		0.33				
A4CA85										The history of the data is known and clear 		0.33				
AD46BD										Owner of data has been identified and can be contacted if needed		0.33				
QFD79B	77	Data Quality				Group	Risk	All datasets being used for this system or model throughout each phase of the lifecycle have been reviewed for consistency following industry best practices (equivalent to ISO 25012)? 		select all that apply:			1	medium	Dataset Quality Guidelines: AI Global  Consistency: The degree to which data has attributes that are free from contradiction and are coherent with other data in a specific context of use. It can be either or both among data regarding one entity and across similar data for comparable entities.	
ADADFF										Data was not reviewed for consistency		-1				
AE1DAB										Dataset values are free from contradition and are coherent with other attributes within the dataset		0.5				
AD497E										Dataset values are free from contradiction and are coherent with other related datasets both produced by your organization or another credible organization 		0.5				
QC13B2		Data Quality				Group	Risk	All datasets being used for this system or model throughout each phase of the lifecycle have been reviewed for currentness/ timeliness following industry best practices (equivalent to ISO 25012)? 		select all that apply:			1	medium	Dataset Quality Guidelines: AI Global   Currentness/timeliness: The degree to which data has attributes that are of the right age in a specific context of use.	
A11013										Data was not reviewed for currentness/ timeliness		-1				
A3E6DE										Dataset is relevant given the time and context it is being used in 		0.33				
AA8F83										The decay of the dataset is understood by system developer and operator 		0.33				
AD7A96										The lifespan of the dataset has been documented clearly in the corresponding metadata		0.33				
QD3128	78	Data Quality				Group	Risk	All datasets being used for this system or model throughout each phase of the lifecycle have been reviewed for confidentiality following industry best practices (equivalent to ISO 25012)? 		select all that apply:			1	medium	Dataset Quality Guidelines: AI Global   Confidentiality: The degree to which data has attributes that ensure that it is only accessible and interpretable by authorized users in a specific context of use. Confidentiality is an aspect of information security (together with availability, integrity) as defined in ISO/IEC 13335-1:2004.	
AE3703										Data was not reviewed for confidentiality 		-1				
AB7B6F										Data is only accessible and interpretable by authorized users in a specific context of use. 		0.33				
A18D36										Data that could be deemed offensive is either removed during use, or an appropriate warning is provided?		0.33				
A28669										Data is anonymized to the greatest extent possible to ensure that no individual directly or indirectly can be identified 		0.33				
QFF2A8	79	Data Quality				Group	Risk	All datasets being used for this system or model throughout each phase of the lifecycle have been reviewed for traceability following industry best practices (equivalent to ISO 25012)? 		select all that apply:			1	medium	Dataset Quality Guidelines:   AI Global   Traceability: The degree to which data has attributes that provide an audit trail of access to the data and of any changes made to the data in a specific context of use.	
AEECC5										Data was not reviewed for traceability		-1				
AF442B										Dataset includes an audit trail of access to the data including any changes made 		0.5				
A1B01B										Dataset values contain unique identifiers where needed 		0.5				
QFD3A6	80	Data Quality				Group	Risk	All datasets being used for this system or model throughout each phase of the lifecycle have been reviewed for compliance following industry best practices (equivalent to ISO 25012)? 		select all that apply:			1	medium	Dataset Quality Guidelines: AI Global   Compliance: The degree to which data has attributes that adhere to standards, conventions or regulations in force and similar rules relating to data quality in a specific context of use.	
A733E7										Data was not reviewed for compliance 		-1				
A359A4										Data complies with all applicable standards 		0.33				
A46F9C										Data complies with all applicable conventions		0.33				
AD00CC										Data complies with all applicable legal rules (laws, regulations, etc) 		0.33				
Q3EFF7	81	Data Quality				Group	Risk	All datasets being used for this system or model throughout each phase of the lifecycle have been reviewed for accessibility including portability and recoverability following industry best practices (equivalent to ISO 25012)?		select all that apply:			1	medium	Dataset Quality Guidelines:  AI Global    Portability: The degree to which data has attributes that enable it to be installed, replaced or moved from one system to another preserving the existing quality in a specific context of use. We can add adaptability under this -Degree to which data can effectively and efficiently be cleansed and / or massaged for different or evolving hardware, software or other operational or usage environments. Replaceability - Degree to which the data can be swapped out by another supplier or newer revisions / updated.  Recoverability: The degree to which data has attributes that enable it to maintain and preserve a specified level of operations and quality, even in the event of failure, in a specific context of use	
A2C000										Data was not reviewed for accessibility		-1				
A75FC8										Data can be accessed by users or operators as needed		0.25				
A78FB0										Data is able to be installed, replaced, or moved from one system to another preserving the existing quality in a specific context or use. 		0.25				
AC87E6										Data is adaptable and cab be swapped out by another supplier or newer revisions/ updates		0.25				
A4ED34										Data is able to be preserved even in the event of a failure		0.25				